# ML Dictionary
This is a dictionary of basic, but important, concepts in Machine Learning.

## Inductive Bias
"_At its core, inductive bias refers to the set of assumptions, constraints, or prior knowledge encoded into a learning algorithm, guiding it to favor certain hypotheses over others._"

See [here 1](https://medium.com/@sanjithkumar986/inductive-bias-in-machine-learning-f360ea678a15) and [here 2](https://medium.com/@sanjithkumar986/inductive-bias-in-deep-learning-1-17a7c3f35381#:~:text=These%20inductive%20biases%20make%20CNNs,tasks%20where%20different%20assumptions%20hold).

## Restrictive Bias
"_Refers to the limitations or constraints imposed on a machine learning model, guiding it to prefer certain hypotheses over others based on the model’s architecture or structure._"

See [here 1](https://medium.com/@sanjithkumar986/inductive-bias-in-machine-learning-f360ea678a15) and [here 2](https://medium.com/@sanjithkumar986/inductive-bias-in-deep-learning-1-17a7c3f35381#:~:text=These%20inductive%20biases%20make%20CNNs,tasks%20where%20different%20assumptions%20hold).

## Preference Bias
"_Involves favoring certain hypotheses over others based on simplicity, interpretability, or prior probabilities, regardless of the model’s architecture._"

See [here 1](https://medium.com/@sanjithkumar986/inductive-bias-in-machine-learning-f360ea678a15) and [here 2](https://medium.com/@sanjithkumar986/inductive-bias-in-deep-learning-1-17a7c3f35381#:~:text=These%20inductive%20biases%20make%20CNNs,tasks%20where%20different%20assumptions%20hold).

## Bias–Variance Tradeoff

"_In statistics and machine learning, the bias–variance tradeoff (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.

The bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).

The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting)._"

See [wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).
